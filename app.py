from flask import Flask, request, jsonify
from flask_cors import CORS
import requests
import os
from dotenv import load_dotenv
from transformers import BertTokenizer, BertForSequenceClassification
import torch
import threading
import time
import logging
from werkzeug.utils import secure_filename
from functools import lru_cache

# è¨­å®šæ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()
OCR_API_KEY = os.environ.get("OCR_API_KEY")

# è¨­å®šå¸¸æ•¸
MAX_FILE_SIZE = 10 * 1024 * 1024  # 10MB
ALLOWED_EXTENSIONS = {'.jpg', '.jpeg', '.png', '.bmp', '.gif'}
MIN_TEXT_LENGTH = 10
DEFAULT_MODEL_PATH = './new_bert_scam_model'
OCR_TIMEOUT = 30
MAX_TEXT_LENGTH = 512

class SpamDetector:
    """
    å–®ä¾‹æ¨¡å¼çš„åƒåœ¾è¨Šæ¯åµæ¸¬å™¨ï¼Œç¢ºä¿åŸ·è¡Œç·’å®‰å…¨ä¸”è³‡æºæ•ˆç‡æœ€ä½³åŒ–
    """
    _instance = None
    _lock = threading.Lock()

    def __new__(cls):
        """å¯¦ä½œ Singleton æ¨¡å¼"""
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
        return cls._instance

    def __init__(self):
        # é¿å…é‡è¤‡åˆå§‹åŒ–
        if hasattr(self, 'initialized'):
            return
            
        self.tokenizer = None
        self.bert_model = None
        self.model_loaded = False
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.initialized = True
        self.model_path = os.environ.get('BERT_MODEL_PATH', DEFAULT_MODEL_PATH)
        
        logger.info(f"âœ… SpamDetector å¯¦ä¾‹å·²å»ºç«‹ (è£ç½®: {self.device})")
        logger.info(f"ğŸ“ æ¨¡å‹è·¯å¾‘: {self.model_path}")

    def _load_model(self):
        """å»¶é²è¼‰å…¥æ¨¡å‹ï¼Œåªåœ¨éœ€è¦æ™‚è¼‰å…¥"""
        if self.model_loaded:
            return True
        
        with self._lock:
            # é›™é‡æª¢æŸ¥é–å®š
            if self.model_loaded:
                return True
                
            start_time = time.time()
            logger.info("ğŸš€ é–‹å§‹è¼‰å…¥ BERT æ¨¡å‹...")
            
            try:
                # æª¢æŸ¥æ¨¡å‹è·¯å¾‘æ˜¯å¦å­˜åœ¨
                if not os.path.exists(self.model_path):
                    logger.error(f"âŒ æ¨¡å‹è·¯å¾‘ä¸å­˜åœ¨: {self.model_path}")
                    return False
                
                # è¼‰å…¥ tokenizer å’Œæ¨¡å‹
                self.tokenizer = BertTokenizer.from_pretrained(self.model_path)
                self.bert_model = BertForSequenceClassification.from_pretrained(
                    self.model_path
                ).to(self.device)
                
                # è¨­å®šç‚ºè©•ä¼°æ¨¡å¼
                self.bert_model.eval()
                self.model_loaded = True
                
                elapsed_time = time.time() - start_time
                logger.info(f"âœ… BERT æ¨¡å‹è¼‰å…¥æˆåŠŸ (è€—æ™‚: {elapsed_time:.2f} ç§’)")
                return True
                
            except Exception as e:
                logger.error(f"âŒ BERT æ¨¡å‹è¼‰å…¥å¤±æ•—ï¼š{e}")
                return False

    @lru_cache(maxsize=128)
    def _predict_cached(self, text_hash):
        """å¿«å–é æ¸¬çµæœä»¥æå‡ç›¸åŒæ–‡å­—çš„è™•ç†é€Ÿåº¦"""
        # æ³¨æ„ï¼šå¯¦éš›ä½¿ç”¨æ™‚éœ€è¦å‚³å…¥åŸå§‹æ–‡å­—è€Œé hash
        pass

    def analyze(self, text, use_cache=True):
        """
        åˆ†ææ–‡å­—æ˜¯å¦ç‚ºè©é¨™è¨Šæ¯
        
        Args:
            text (str): è¦åˆ†æçš„æ–‡å­—
            use_cache (bool): æ˜¯å¦ä½¿ç”¨å¿«å–
            
        Returns:
            dict: åˆ†æçµæœ
        """
        # ç¢ºä¿æ¨¡å‹å·²è¼‰å…¥
        if not self._load_model():
            logger.error("æ¨¡å‹è¼‰å…¥å¤±æ•—")
            return {
                'error': 'æ¨¡å‹æœªèƒ½æˆåŠŸè¼‰å…¥ï¼Œç„¡æ³•åˆ†æ',
                'code': 'MODEL_LOAD_ERROR'
            }

        # æ–‡å­—é è™•ç†
        text = text.strip()
        if len(text) > MAX_TEXT_LENGTH:
            logger.warning(f"æ–‡å­—é•·åº¦ {len(text)} è¶…éä¸Šé™ï¼Œå°‡é€²è¡Œæˆªæ–·")
            text = text[:MAX_TEXT_LENGTH]

        try:
            # Tokenize è¼¸å…¥æ–‡å­—
            inputs = self.tokenizer(
                text, 
                return_tensors="pt", 
                truncation=True, 
                padding=True, 
                max_length=128
            ).to(self.device)
            
            # é€²è¡Œé æ¸¬
            with torch.no_grad():
                outputs = self.bert_model(**inputs)
            
            # è¨ˆç®—æ©Ÿç‡
            logits = outputs.logits
            probabilities = torch.softmax(logits, dim=-1)
            spam_score = probabilities[0][1].item()
            
            # æ ¼å¼åŒ–è¼¸å‡º
            confidence_percentage = f"{spam_score * 100:.1f}%"
            
            # æ ¹æ“šé–¾å€¼åˆ¤æ–·
            thresholds = {
                'high': 0.8,
                'medium': 0.5,
                'low': 0.2
            }
            
            if spam_score >= thresholds['high']:
                risk_level = 'é«˜'
                display_text = f"âš ï¸ é«˜é¢¨éšªè­¦å‘Šï¼šæ­¤è¨Šæ¯æœ‰ {confidence_percentage} çš„æ©Ÿç‡ç‚ºè©é¨™ã€‚å¼·çƒˆå»ºè­°æ’¥æ‰“ 165 åè©é¨™å°ˆç·šã€‚"
                is_scam = True
            elif spam_score >= thresholds['medium']:
                risk_level = 'ä¸­'
                display_text = f"âš ï¸ ä¸­é¢¨éšªæé†’ï¼šæ­¤è¨Šæ¯æœ‰ {confidence_percentage} çš„æ©Ÿç‡ç‚ºè©é¨™ã€‚å»ºè­°è¬¹æ…è™•ç†ã€‚"
                is_scam = True
            else:
                risk_level = 'ä½'
                display_text = f"âœ… æ­¤è¨Šæ¯ç‚ºè©é¨™çš„å¯èƒ½æ€§ç‚º {confidence_percentage}ï¼Œé¢¨éšªè¼ƒä½ã€‚"
                is_scam = False

            result = {
                "success": True,
                "display_text": display_text,
                "is_scam": is_scam,
                "risk_level": risk_level,
                "confidence_percentage": confidence_percentage,
                "raw_score": round(spam_score, 4),
                "text_length": len(text),
                "model_version": "BERT-v1.0",
                "analysis_timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
            }
            
            logger.info(f"åˆ†æå®Œæˆ - é¢¨éšªç­‰ç´š: {risk_level}, åˆ†æ•¸: {confidence_percentage}")
            return result
            
        except Exception as e:
            logger.error(f"âŒ BERT é æ¸¬éŒ¯èª¤: {e}", exc_info=True)
            return {
                'error': f'æ¨¡å‹é æ¸¬æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}',
                'code': 'PREDICTION_ERROR'
            }

def validate_image(image_file):
    """
    é©—è­‰ä¸Šå‚³çš„åœ–ç‰‡æª”æ¡ˆ
    
    Args:
        image_file: Flask æª”æ¡ˆç‰©ä»¶
        
    Returns:
        tuple: (æ˜¯å¦æœ‰æ•ˆ, éŒ¯èª¤è¨Šæ¯)
    """
    if not image_file:
        return False, "æœªæä¾›åœ–ç‰‡æª”æ¡ˆ"
    
    # æª¢æŸ¥æª”æ¡ˆåç¨±
    filename = secure_filename(image_file.filename)
    if not filename:
        return False, "ç„¡æ•ˆçš„æª”æ¡ˆåç¨±"
    
    # æª¢æŸ¥å‰¯æª”å
    ext = os.path.splitext(filename)[1].lower()
    if ext not in ALLOWED_EXTENSIONS:
        return False, f"ä¸æ”¯æ´çš„æª”æ¡ˆæ ¼å¼ã€‚æ”¯æ´æ ¼å¼: {', '.join(ALLOWED_EXTENSIONS)}"
    
    # æª¢æŸ¥æª”æ¡ˆå¤§å°
    image_file.seek(0, os.SEEK_END)
    file_size = image_file.tell()
    image_file.seek(0)
    
    if file_size > MAX_FILE_SIZE:
        return False, f"æª”æ¡ˆå¤§å°è¶…éé™åˆ¶ ({MAX_FILE_SIZE // 1024 // 1024}MB)"
    
    return True, None

def perform_ocr(image_file):
    """
    åŸ·è¡Œ OCR è™•ç†ï¼Œè¿”å›æå–çš„æ–‡å­—
    
    Args:
        image_file: Flask æª”æ¡ˆç‰©ä»¶
        
    Returns:
        tuple: (æå–çš„æ–‡å­—, éŒ¯èª¤å­—å…¸)
    """
    if not OCR_API_KEY:
        logger.error("OCR_API_KEY æœªè¨­å®š")
        return None, {
            'error': 'OCR æœå‹™æœªè¨­å®š',
            'code': 'OCR_API_KEY_MISSING'
        }

    try:
        # é©—è­‰åœ–ç‰‡
        is_valid, error_msg = validate_image(image_file)
        if not is_valid:
            return None, {'error': error_msg, 'code': 'INVALID_IMAGE'}
        
        # æº–å‚™æª”æ¡ˆä¸Šå‚³
        ext = os.path.splitext(image_file.filename)[1].lower()
        mime_types = {
            '.jpg': 'image/jpeg', 
            '.jpeg': 'image/jpeg', 
            '.png': 'image/png', 
            '.bmp': 'image/bmp', 
            '.gif': 'image/gif'
        }
        mime = mime_types.get(ext, 'image/jpeg')
        
        # é‡ç½®æª”æ¡ˆæŒ‡é‡
        image_file.seek(0)
        
        # ç™¼é€ OCR è«‹æ±‚
        logger.info(f"ç™¼é€ OCR è«‹æ±‚ (æª”æ¡ˆé¡å‹: {mime})")
        
        ocr_response = requests.post(
            'https://api.ocr.space/parse/image',
            files={'filename': (f'image{ext}', image_file, mime)},
            data={
                'apikey': OCR_API_KEY,
                'language': 'cht',
                'isOverlayRequired': False,
                'detectOrientation': True,
                'scale': True,
                'OCREngine': 2  # ä½¿ç”¨ OCR Engine 2 ä»¥ç²å¾—æ›´å¥½çš„ä¸­æ–‡è­˜åˆ¥
            },
            timeout=OCR_TIMEOUT
        )
        
        # æª¢æŸ¥ HTTP ç‹€æ…‹ç¢¼
        if ocr_response.status_code != 200:
            logger.error(f"OCR API å›æ‡‰éŒ¯èª¤: HTTP {ocr_response.status_code}")
            return None, {
                'error': f'OCR æœå‹™æš«æ™‚ç„¡æ³•ä½¿ç”¨ (HTTP {ocr_response.status_code})',
                'code': 'OCR_HTTP_ERROR'
            }
        
        # è§£æå›æ‡‰
        result = ocr_response.json()
        
        # æª¢æŸ¥è™•ç†éŒ¯èª¤
        if result.get('IsErroredOnProcessing'):
            error_details = (
                result.get('ErrorMessage') or 
                result.get('ErrorDetails') or 
                'æœªçŸ¥éŒ¯èª¤'
            )
            logger.error(f"OCR è™•ç†éŒ¯èª¤: {error_details}")
            return None, {
                'error': 'OCR è™•ç†å¤±æ•—',
                'details': error_details,
                'code': 'OCR_PROCESSING_ERROR'
            }
        
        # æå–æ–‡å­—
        parsed_results = result.get('ParsedResults', [])
        if not parsed_results:
            logger.warning("OCR ç„¡æ³•æå–æ–‡å­—")
            return None, {
                'error': 'ç„¡æ³•å¾åœ–ç‰‡ä¸­è­˜åˆ¥å‡ºæ–‡å­—ï¼Œè«‹ç¢ºä¿åœ–ç‰‡æ¸…æ™°ä¸”åŒ…å«æ–‡å­—',
                'code': 'NO_TEXT_FOUND'
            }
        
        extracted_text = parsed_results[0].get('ParsedText', '').strip()
        
        # è¨˜éŒ„çµæœ
        logger.info(f"OCR æˆåŠŸæå– {len(extracted_text)} å€‹å­—å…ƒ")
        
        return extracted_text, None
        
    except requests.exceptions.Timeout:
        logger.error("OCR API è«‹æ±‚è¶…æ™‚")
        return None, {
            'error': 'OCR æœå‹™å›æ‡‰è¶…æ™‚ï¼Œè«‹ç¨å¾Œå†è©¦',
            'code': 'OCR_TIMEOUT'
        }
    except requests.exceptions.RequestException as e:
        logger.error(f"OCR API è«‹æ±‚éŒ¯èª¤: {e}")
        return None, {
            'error': 'OCR æœå‹™é€£ç·šå¤±æ•—',
            'details': str(e),
            'code': 'OCR_REQUEST_ERROR'
        }
    except Exception as e:
        logger.error(f"OCR è™•ç†æœªé æœŸéŒ¯èª¤: {e}", exc_info=True)
        return None, {
            'error': 'è™•ç†åœ–ç‰‡æ™‚ç™¼ç”ŸéŒ¯èª¤',
            'details': str(e),
            'code': 'OCR_UNEXPECTED_ERROR'
        }

# ------------------- Flask æ‡‰ç”¨ç¨‹å¼ -------------------
app = Flask(__name__)
CORS(app, origins=['http://localhost:3000', 'https://yourdomain.com'])  # é™åˆ¶ CORS ä¾†æº

# è¨­å®šæª”æ¡ˆä¸Šå‚³é™åˆ¶
app.config['MAX_CONTENT_LENGTH'] = MAX_FILE_SIZE

# åˆå§‹åŒ–åµæ¸¬å™¨
detector_instance = SpamDetector()

@app.errorhandler(413)
def request_entity_too_large(error):
    """è™•ç†æª”æ¡ˆéå¤§éŒ¯èª¤"""
    return jsonify({
        'error': f'æª”æ¡ˆå¤§å°è¶…éé™åˆ¶ ({MAX_FILE_SIZE // 1024 // 1024}MB)',
        'code': 'FILE_TOO_LARGE'
    }), 413

@app.errorhandler(500)
def internal_server_error(error):
    """è™•ç†å…§éƒ¨ä¼ºæœå™¨éŒ¯èª¤"""
    logger.error(f"å…§éƒ¨ä¼ºæœå™¨éŒ¯èª¤: {error}")
    return jsonify({
        'error': 'ä¼ºæœå™¨å…§éƒ¨éŒ¯èª¤ï¼Œè«‹ç¨å¾Œå†è©¦',
        'code': 'INTERNAL_ERROR'
    }), 500

@app.route('/', methods=['GET'])
def health_check():
    """å¥åº·æª¢æŸ¥ç«¯é»"""
    status = {
        "status": "OK",
        "message": "åè©é¨™ API æ­£å¸¸é‹ä½œ",
        "version": "2.0",
        "model_loaded": detector_instance.model_loaded,
        "ocr_enabled": bool(OCR_API_KEY),
        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
    }
    return jsonify(status), 200

@app.route('/analyze-text', methods=['POST'])
def analyze_text():
    """ç´”æ–‡å­—åˆ†æç«¯é»"""
    try:
        # å–å¾—è«‹æ±‚è³‡æ–™
        data = request.get_json()
        if not data:
            return jsonify({
                'error': 'è«‹æä¾› JSON æ ¼å¼çš„è³‡æ–™',
                'code': 'INVALID_REQUEST'
            }), 400
        
        text = data.get('text', '').strip()
        
        # é©—è­‰æ–‡å­—
        if not text:
            return jsonify({
                'error': 'è«‹æä¾›è¦åˆ†æçš„æ–‡å­—',
                'code': 'NO_TEXT'
            }), 400
        
        if len(text) < MIN_TEXT_LENGTH:
            return jsonify({
                'error': f'æ–‡å­—é•·åº¦è‡³å°‘éœ€è¦ {MIN_TEXT_LENGTH} å€‹å­—å…ƒ',
                'code': 'TEXT_TOO_SHORT'
            }), 400
        
        # åŸ·è¡Œåˆ†æ
        result = detector_instance.analyze(text)
        
        if 'error' in result:
            return jsonify(result), 500
        
        return jsonify(result), 200
        
    except Exception as e:
        logger.error(f"åˆ†ææ–‡å­—æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)
        return jsonify({
            'error': 'è™•ç†è«‹æ±‚æ™‚ç™¼ç”ŸéŒ¯èª¤',
            'details': str(e),
            'code': 'PROCESSING_ERROR'
        }), 500

@app.route('/analyze-image', methods=['POST'])
def analyze_image():
    """åœ–ç‰‡ OCR åˆ†æç«¯é»"""
    try:
        # æª¢æŸ¥æ˜¯å¦æœ‰åœ–ç‰‡
        if 'image' not in request.files:
            return jsonify({
                'error': 'è«‹ä¸Šå‚³åœ–ç‰‡æª”æ¡ˆ',
                'code': 'NO_IMAGE'
            }), 400
        
        image_file = request.files['image']
        
        # åŸ·è¡Œ OCR
        extracted_text, error = perform_ocr(image_file)
        
        if error:
            return jsonify(error), 400
        
        # æª¢æŸ¥æå–çš„æ–‡å­—
        if len(extracted_text) < MIN_TEXT_LENGTH:
            return jsonify({
                'error': f'åœ–ç‰‡ä¸­è­˜åˆ¥çš„æ–‡å­—å¤ªå°‘ï¼ˆæœ€å°‘éœ€è¦ {MIN_TEXT_LENGTH} å€‹å­—å…ƒï¼‰ï¼Œè«‹ä¸Šå‚³æ›´æ¸…æ™°çš„åœ–ç‰‡',
                'code': 'INSUFFICIENT_TEXT'
            }), 400
        
        # åŸ·è¡Œåˆ†æ
        result = detector_instance.analyze(extracted_text)
        result['source'] = 'image_ocr'
        result['extracted_text'] = extracted_text[:200] + '...' if len(extracted_text) > 200 else extracted_text
        
        if 'error' in result:
            return jsonify(result), 500
        
        return jsonify(result), 200
        
    except Exception as e:
        logger.error(f"åˆ†æåœ–ç‰‡æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)
        return jsonify({
            'error': 'è™•ç†åœ–ç‰‡æ™‚ç™¼ç”ŸéŒ¯èª¤',
            'details': str(e),
            'code': 'IMAGE_PROCESSING_ERROR'
        }), 500

@app.route('/analyze-all', methods=['POST'])
def analyze_all():
    """
    ç¶œåˆåˆ†æç«¯é»ï¼šæ”¯æ´åœ–ç‰‡ + æ–‡å­—
    """
    try:
        # å–å¾—è¼¸å…¥è³‡æ–™
        image_file = request.files.get('image', None)
        text_input = request.form.get('text', '').strip()
        
        extracted_text = ''
        ocr_performed = False
        
        # è™•ç†åœ–ç‰‡ OCRï¼ˆå¦‚æœæœ‰æä¾›ï¼‰
        if image_file:
            logger.info("è™•ç†ä¸Šå‚³çš„åœ–ç‰‡...")
            extracted_text, error = perform_ocr(image_file)
            
            if error:
                # OCR å¤±æ•—ä½†æœ‰æ–‡å­—è¼¸å…¥æ™‚ï¼Œç¹¼çºŒè™•ç†æ–‡å­—
                if text_input:
                    logger.warning(f"OCR å¤±æ•—ä½†æœ‰æ–‡å­—è¼¸å…¥ï¼Œç¹¼çºŒè™•ç†: {error}")
                else:
                    return jsonify(error), 400
            else:
                ocr_performed = True
                
                # æª¢æŸ¥ OCR çµæœ
                if len(extracted_text) < MIN_TEXT_LENGTH and not text_input:
                    return jsonify({
                        'error': f'åœ–ç‰‡è¾¨è­˜çš„æ–‡å­—å¤ªå°‘ï¼Œè«‹é‡æ–°ä¸Šå‚³æ›´æ¸…æ™°çš„åœ–ç‰‡æˆ–è£œå……æ–‡å­—èªªæ˜',
                        'code': 'INSUFFICIENT_OCR_TEXT'
                    }), 400
        
        # åˆä½µæ–‡å­—
        full_text = ' '.join(filter(None, [extracted_text, text_input])).strip()
        
        # é©—è­‰åˆä½µå¾Œçš„æ–‡å­—
        if not full_text:
            return jsonify({
                'error': 'è«‹æä¾›åœ–ç‰‡æˆ–æ–‡å­—é€²è¡Œåˆ†æ',
                'code': 'NO_CONTENT'
            }), 400
        
        if len(full_text) < MIN_TEXT_LENGTH:
            return jsonify({
                'error': f'ç¸½æ–‡å­—é•·åº¦è‡³å°‘éœ€è¦ {MIN_TEXT_LENGTH} å€‹å­—å…ƒ',
                'code': 'COMBINED_TEXT_TOO_SHORT'
            }), 400
        
        # åŸ·è¡Œåˆ†æ
        logger.info(f"é–‹å§‹åˆ†ææ–‡å­— (ç¸½é•·åº¦: {len(full_text)} å­—å…ƒ)")
        analysis_result = detector_instance.analyze(full_text)
        
        # æ·»åŠ é¡å¤–è³‡è¨Š
        analysis_result['sources'] = {
            'has_image': bool(image_file),
            'has_text': bool(text_input),
            'ocr_performed': ocr_performed,
            'total_length': len(full_text)
        }
        
        if ocr_performed and extracted_text:
            # åªé¡¯ç¤ºéƒ¨åˆ† OCR æ–‡å­—ä»¥ä¿è­·éš±ç§
            analysis_result['ocr_preview'] = (
                extracted_text[:100] + '...' 
                if len(extracted_text) > 100 
                else extracted_text
            )
        
        if 'error' in analysis_result:
            return jsonify(analysis_result), 500

        return jsonify(analysis_result), 200

    except Exception as e:
        logger.error(f"ç¶œåˆåˆ†ææ™‚ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)
        return jsonify({
            'error': 'è™•ç†è«‹æ±‚æ™‚ç™¼ç”ŸéŒ¯èª¤',
            'details': str(e),
            'code': 'GENERAL_ERROR'
        }), 500

@app.route('/model-info', methods=['GET'])
def model_info():
    """å–å¾—æ¨¡å‹è³‡è¨Š"""
    info = {
        "model_loaded": detector_instance.model_loaded,
        "model_path": detector_instance.model_path,
        "device": str(detector_instance.device),
        "ocr_enabled": bool(OCR_API_KEY),
        "supported_formats": list(ALLOWED_EXTENSIONS),
        "max_file_size_mb": MAX_FILE_SIZE // 1024 // 1024,
        "min_text_length": MIN_TEXT_LENGTH,
        "max_text_length": MAX_TEXT_LENGTH
    }
    return jsonify(info), 200

if __name__ == '__main__':
    # ç”Ÿç”¢ç’°å¢ƒè¨­å®š
    port = int(os.environ.get('PORT', 5000))
    debug = os.environ.get('FLASK_ENV') == 'development'
    
    logger.info(f"ğŸš€ å•Ÿå‹•åè©é¨™ API æœå‹™ (Port: {port}, Debug: {debug})")
    
    # æ³¨æ„ï¼šç”Ÿç”¢ç’°å¢ƒæ‡‰ä½¿ç”¨ WSGI ä¼ºæœå™¨å¦‚ Gunicorn
    app.run(
        debug=debug,
        host='0.0.0.0',
        port=port,
        threaded=True  # å•Ÿç”¨å¤šåŸ·è¡Œç·’
    )